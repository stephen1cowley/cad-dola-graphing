{
    "llm_name": "huggyllama/llama-13b",
    "device": "cuda",
    "context_prompt": "Instruction: read the given information and answer the corresponding question.\n\n{context}\nQ: {question}\nA:",
    "no_context_prompt": "Instruction: answer the corresponding question.\n\nQ:{question}\nA:",
    "decoding_strategy": "CAD",
    "test_coefficients": [0.0, 1.0],
    "apc": 0.0,
    "dola_layers_context": "none",
    "dola_layers_no_context": "none",
    "max_tokens": 15,
    "max_hours": 3.0,
    "dataset_path": "../nq/orig_dev_filtered.json",
    "output_file": "../results/exp_13b_my_prompt.json"
}
